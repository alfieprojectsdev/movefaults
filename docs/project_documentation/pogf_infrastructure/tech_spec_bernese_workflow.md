# Technical Specification: Automated Bernese Processing Workflow

**Version:** 1.0

**Date:** 2026-01-25

## 1. Introduction

### 1.1. Purpose

This document outlines the technical specifications for the Automated Bernese Processing Workflow. The system's primary goal is to orchestrate the execution of Bernese processing campaigns (sets of observation data processed together) in a fully automated fashion. This workflow handles **RINEX data derived from proprietary raw GNSS observations** (from both Campaign GPS and Continuous GPS observation methods) for precise scientific analysis, guided by the official Bernese v5.2 documentation. **It does not directly process VADASE real-time monitoring data (NMEA formats).**

### 1.2. Scope

This specification details the system's architecture, the stages of the processing workflow, the technology stack, and strategies for configuration and error handling. It assumes the Bernese GNSS Software v5.2+ is installed and licensed on the execution machine, and its operation is informed by the comprehensive documentation found in `/home/finch/repos/movefaults/docs/bern52/`, particularly the "BERN52 Guide (detailed version).pdf". The system is designed to seamlessly process **RINEX data streams** from diverse GNSS observation methods, focusing on high-precision geodetic products.

## 2. System Architecture

The workflow is designed as a long-running, asynchronous task, managed by a task queue. This architecture decouples the processing from the initial trigger, allowing for robust, scalable, and monitorable execution.

The system is triggered by a "start processing" event, which can be initiated manually or on a schedule. The orchestrator then executes a series of steps, interacting with the Centralized Geodetic Database and the local file system where Bernese is installed.

![Bernese Workflow Architecture](https://i.imgur.com/example3.png "Diagram showing a trigger starting a Celery task. The task reads from the Database, writes Bernese config files to disk, calls the BPE, then parses results and writes them back to the Database.")

## 3. Workflow Stages

### 3.1. Stage 1: Triggering

- **Responsibility:** To initiate a new processing run.
- **Implementation:** Triggers can be:
  - **Manual:** An operator initiates a run via a command-line interface or a future admin dashboard.
  - **Scheduled:** A cron job or Celery Beat schedule starts a run at a defined interval (e.g., daily at 23:00 for final products).
  - **Event-Driven:** A run is automatically started once a certain threshold of new data has been ingested.
- **Output:** A task is added to the Celery queue with parameters defining the run (e.g., date range, processing profile).

### 3.2. Stage 2: Data Preparation

- **Responsibility:** To gather all necessary input data for the Bernese campaign.
- **Implementation:** The worker fetches the following from the Centralized Geodetic Database:
  - A list of RINEX files for the given time period.
  - Station metadata (coordinates, antenna types) for all relevant stations.
  - IGS products (orbits, clocks) required for the run.
- **Output:** All necessary data is organized in a temporary campaign directory on the file system.

### 3.3. Stage 3: Configuration Generation

- **Responsibility:** To create the specific configuration files required by the Bernese Processing Engine (BPE), as detailed in the Bernese v5.2 documentation (e.g., sections in "BERN52 Guide (detailed version).pdf" or relevant `DOCU52_part_xxx.pdf` files describing input formats).
- **Implementation:** The system uses the Jinja2 templating engine to generate files from a set of predefined templates. This includes:
  - **PCF (Process Control File):** The main script defining the processing steps. Different templates will exist for different processing profiles (e.g., `rapid`, `final`, `ionosphere`). The structure and syntax will strictly follow Bernese specifications.
  - **STA (Station Information File):** Contains station coordinates and other metadata.
  - **Panel Files:** Any other input files required by the PCF.
- **Output:** A complete and valid set of Bernese campaign input files is written to the campaign directory.

### 3.4. Stage 4: BPE Execution

- **Responsibility:** To execute the Bernese Processing Engine and monitor its progress, as described in the BPE chapters of the Bernese v5.2 documentation (e.g., sections in "BERN52 Guide (detailed version).pdf" or related `All_Modules_Combined_part_xxx.pdf` files).
- **Implementation:** The Python script calls the BPE executable (`BPE.exe` or equivalent) using the `subprocess` module.
  - It passes the campaign name and other relevant arguments.
  - It captures `stdout` and `stderr` from the BPE process in real-time and logs them.
  - It monitors the exit code of the BPE process to determine success or failure.
- **Output:** The raw output files generated by Bernese are stored in the campaign's `OUT` directory. A log of the execution is saved.

### 3.5. Stage 5: Results Parsing & Loading

- **Responsibility:** To extract meaningful results from the Bernese output files and load them into the database.
- **Implementation:** After a successful BPE run, a set of parsers will read the relevant output files:
  - **`.CRD` / `.TRP` files:** For station coordinates and tropospheric data.
  - **`.VEL` files:** For velocity estimations.
  The parsed data is structured and loaded into the `timeseries_data` and `velocity_products` tables in the Centralized Geodetic Database using SQLAlchemy.
- **Output:** New records are added to the database, and the temporary campaign directory is archived.

## 4. Technology Stack

- **Orchestration:** Python 3.11+ with Celery.
  - **Justification:** Reuses the existing task queue infrastructure. Ideal for managing long-running, resource-intensive tasks.
- **Templating:** Jinja2.
  - **Justification:** A powerful and widely-used templating engine that is perfect for generating structured text files like Bernese configurations.
- **BPE Interaction:** Python's built-in `subprocess` module.
  - **Justification:** Provides direct, low-level control over the execution of the command-line BPE, which is sufficient for this use case.
- **Database Interaction:** SQLAlchemy 2.0.
  - **Justification:** Consistency with the Ingestion Pipeline.

## 5. Error Handling

- **BPE Failures:** If the BPE process returns a non-zero exit code, the Celery task will be marked as failed. The captured logs will be preserved for debugging.
- **Parsing Errors:** If a parser fails to read an output file, the error will be logged, and the loading step for that specific product will be skipped.
- **Database Errors:** Transactional inserts will be used to ensure that results are loaded atomically. On failure, the task will be retried.
